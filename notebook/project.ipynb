{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimentation\n",
    "\n",
    "### 3.1 Experimental Procedures\n",
    "For the experimental procedure, we first trained all the attention models once, then trained the same models but applied data augmentation to the models. After this, we merged all the results into one graph to analyse our findings. \n",
    "\n",
    "##### 3.1.1 Experimental Parameters\n",
    "To ensure the reliability of our experiment, we have opted to run all our attention models in one server for more control and enforce validity of our experiment through training all models with the same parameters as seen here.\n",
    "\n",
    "We have chosen the epoch to be 400 because we wanted to ensure our models were still improving and that there is no validation loss despite potential overfitting towards some light models like mobileNet.\n",
    "\n",
    "##### 3.1.2 Training\n",
    "We used \"graphic card\" \n",
    "\n",
    "### 3.2 Evaluation Metrics\n",
    "\n",
    "\n",
    "##### 3.2.1 Intersection over Union (lOU)\n",
    "Intersection over Union measures the intersection between predicted bounding box and the ground truth. It helps quantifie the degree of accuracy for the prediction. \n",
    "\n",
    "With a threshold, we can define positive if loU greater than threshold, a negative vice versa. \n",
    "<img src=\"lou.png\" style=\"width:682px;height:865px\">\n",
    "\n",
    "\n",
    "\n",
    "##### 3.2.2 Qualitative Analysis of Prediction\n",
    "**Precision** is the proportion of the amount of true positives over all the true positives + false positives and is a measure of how accurate your prediction is.\n",
    "\n",
    "**Recall** is the proportion of true positives over the true positives plus the false negatives and is a measure of how well you find all the positives.\n",
    "\n",
    "\n",
    "##### 3.2.3 mAP\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### 3.3 YOLOv5\n",
    "Yolo, also known as you only look once, it’s one of the most popular, powerful algorithms in the field of object detection, due to its fast and efficient performance. YOLOv5 is the latest generation of YOLO series of algorithm. \n",
    "\n",
    "YOLOv5 has multiple varieties of pre-trained models, the difference is the trade off between network size and final performance. We choose YOLOv5l as our base model as it provide good balance between effciency and performance. \n",
    "\n",
    "<img src=\"yolov5Models.png\" style=\"width:760px;height:378px\">\n",
    "\n",
    "##### 3.3.1 Object Detection\n",
    "Object detection is the task of detecting an instance of an object, and classify to a particular class inside an image. \n",
    "<img src=\"label.png\" style=\"width:503px;height:503px\">\n",
    "\n",
    "##### 3.3.3 YOLOv5 Mechanism\n",
    "YOLO reframes object detection as a regression problem. It takes an input image and learns to predict multiple bounding boxes, with a confidence score , then does a specific object classification. \n",
    "\n",
    "To do so, the image is divided into ‘S’ X ‘S’ grids, then if the centre of an object is in one of these grids, the grid has bounding boxes is responsible for detecting that if objects are within the grid through. Each grid then is responsible for predicting B bounding boxes, and a confidence score indicating how confident the network thinks if there is an object present in the box. \n",
    "\n",
    "<img src=\"yolo_Mechanism.jpg\" style=\"width:623px;height:372px\">\n",
    "\n",
    "The confidence score detects  there is no object in the box, the confidence score should be zero. Otherwise, it would be equal to the intersection over union (loU) between box and ground truth. For each box, YOLO uses a CNN based googleNet to derive a class specific probability. The final confidence score will be the product of conditional class specific probability and individual box confidence score. Tell us both the probability of the class appearing in the box, and how well the predicted box fits the object. Then it apply threshold value to eliminate low confidence prediction.\n",
    "\n",
    "<img src=\"model2.png\" style=\"width:1715px;height:469px\">\n",
    "\n",
    "##### 3.3.4 YOLOv5 Model Architecture\n",
    "YOLO network architecture consists of three parts, backbone, which will extract unique features from the particular image. Neck, elaborate in feature Pyramid. Then head is responsible for generating the result, the graph with bounding boxes, confidence score and object classification.\n",
    "\n",
    "In YOLOv5, it uses CSPDarknet as its backbone, which is a CNN  that adopts cross stage partial structure. It partitions the feature map of the layer into two parts and merges them through a cross-stage hierarchy. By split and merge strategy it optimises duplicate gradient information while maintaining accuracy. Then utilise PANet as its neck, downsampling the extracted feature, obtains a feature map with decreasing size and pixels, it enhances recognition ability and performs better detection and helps detect objects in all scales. Then head layer for the final output.  \n",
    "\n",
    "<img src=\"networkArchitecture.png\" style=\"width:850px;height:634px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Attention Modules\n",
    "Attention is an attempt to enhance the important parts while downplaying irrelevant information. In our dataset, we focus on Pedestrian detection, Weakens other non-relevant information, such as traffic signals.\n",
    "\n",
    "#### 3.4.1 SE\n",
    "SE net also known as the Squeeze-and-Excitation Networks consists of 3 steps: the Squeeze Module, Excitation Module, and Scale Module. \n",
    "<img src=\"se.jpg\" style=\"width:1116px;height:227px\">\n",
    "\n",
    "First, the squeeze excitation block takes an input tensor x reduces it to a (C × 1 × 1) tensor through passing the  ( Global Average Pool) layer.\n",
    "\n",
    "Next is the Excitation module. The output (C×1×1) tensor from the last section is passed to a c-length vector layer and outputs a tensor of same length that is then broadcasted and multiplied lament-wise with the input x.\n",
    "\n",
    "SE is a well renowned attention mechanism and adding it in front of the SPPP layer in the yolov5 backbone as suggested by the author, could increase performance. \n",
    "\n",
    "<img src=\"se2.jpg\" style=\"width:248px;height:331px\">\n",
    "\n",
    "#### 3.4.2 SPD\n",
    "From the diagram, we can see that the SPD-Conv net consists of two components: a  space to depth layer followed by a non-strided convolution layer. \n",
    "\n",
    "The SPD layer downsamples a feature map X but retains all the information in the channel dimension, and thus there is no information loss. \n",
    "\n",
    "The non-strided convolution operation after each SPD to reduce the (increased) number of channels using learnable parameters in the added convolution layer.\n",
    "\n",
    "We have chosen SPD attention module because the spd paper noted that it improves performance of images with low resolutions of size 640 x 640 and this fits with our dataset.\n",
    "\n",
    "<img src=\"spd.jpg\" style=\"width:761px;height:390px\">\n",
    "\n",
    "#### ECA\n",
    "\n",
    "#### Mobile Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d163f16b58e91b4f0c6552c673da9b206828735c514af5e7a3bab2c6fbea8921"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
